{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bddbc63b",
   "metadata": {},
   "source": [
    "### Preparing Data for Apache Spark MLÂ Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b7c735",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "Apache Spark is an open-source tool for large-scale data processing. PySpark allows to write spark applications using Python APIs. PySpark contains many libraries for writing effieicnt programs. Some other external libarries are PySpark SQL for processing structured and semi-strucutred data, streaming, MLLib for machine learning and GraphX for graph computation.\n",
    "\n",
    "\n",
    "Before we feed data into machine learning models in PySpark, we need to assemble individual variables into a single feature vector. This is done by using Spark's VectorAssembler. VectorAssembler takes a list of columns as input and combines them into a single vector column. It is useful for combining various raw as well as generated/transformed features into a single feature vector which then can be used for modeling. \n",
    "\n",
    "PySpark modeling consumes data differently than others such as in Scikit learn modeling. Therefore, it is important to know how to process data in spark framework and prepare it for modeling. The main purpose of this article is to exactly do so. \n",
    "\n",
    "While working with data, we may encounter both categorical or numerical variables . This article summarizes the steps required to handle both categorical and numerical varialbes and how to convert them to a single feature vector, which later then can be used for modeling purpose. \n",
    "\n",
    "This post specifically covers:\n",
    "\n",
    "    > Creating a sample spark dataframe\n",
    "    \n",
    "    > Using VectorAssembler when only numerical features are avaialble\n",
    "    \n",
    "    > Using VectorAssembler when both numerical and categorical features are avaialble"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077ebcbc",
   "metadata": {},
   "source": [
    "#### Process flow \n",
    "The overall data processing steps in spark starts with data ingestion, conversion of categorical variable to numeric form and using it for vectorization. Once both categorical and numerical variable are assmebled in a vectorized form, it can further be divided into train/test/validation data sets and use it for modeling purpose. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8756c05b",
   "metadata": {},
   "source": [
    "### Import librariers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31331dbf",
   "metadata": {},
   "source": [
    "Let's import required libaries and start a spark session and name this as VectAssembler application. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d3b57ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('VectAssembler').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33dda336",
   "metadata": {},
   "source": [
    "#### Scenario 1:  Vector Assembler when only numerical features are available\n",
    "Below we create a spark dataframe that consists of 3 numerical variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "375cda25",
   "metadata": {},
   "outputs": [],
   "source": [
    "customerdata = spark.createDataFrame(\n",
    "    [\n",
    "        (1, 29.99, 5),\n",
    "        (2, 31.99, 3),\n",
    "        (3, 24.99, 1),\n",
    "        (4, 21.99, 3),\n",
    "    ],[\"cust_id\", 'monthly_payment', 'tenure_yrs']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac4fc628",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------------+----------+\n",
      "|cust_id|monthly_payment|tenure_yrs|\n",
      "+-------+---------------+----------+\n",
      "|      1|          29.99|         5|\n",
      "|      2|          31.99|         3|\n",
      "|      3|          24.99|         1|\n",
      "+-------+---------------+----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customerdata.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a689bbb",
   "metadata": {},
   "source": [
    "The VectorAssembler combines all the variables into one column. Let's call it 'features' as shown below. This is an important step because the features column now becomes the input for machine learning model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bef0b388",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"monthly_payment\", \"tenure_yrs\"],\n",
    "    outputCol=\"features\")\n",
    "\n",
    "outdata = assembler.transform(customerdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb82469c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------------+----------+-----------+\n",
      "|cust_id|monthly_payment|tenure_yrs|features   |\n",
      "+-------+---------------+----------+-----------+\n",
      "|1      |29.99          |5         |[29.99,5.0]|\n",
      "|2      |31.99          |3         |[31.99,3.0]|\n",
      "|3      |24.99          |1         |[24.99,1.0]|\n",
      "|4      |21.99          |3         |[21.99,3.0]|\n",
      "+-------+---------------+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "outdata.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef9695c",
   "metadata": {},
   "source": [
    "#### Scenario 2: Vector Assembler when both numerical and categorical features are available\n",
    "Below I have created a spark dataframe consisting of 3 numerical (cust_id, monthly_payment and tenure_yrs) and 1 categorical (state) variables. The state variable contains 3 types: MD, VA and WA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07b62d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "customerdata1 = spark.createDataFrame(\n",
    "    [\n",
    "        (1, 29.99, 5, 'MD'),\n",
    "        (2, 31.99, 3, 'MD'),\n",
    "        (3, 24.99, 1, 'VA'),\n",
    "        (4, 21.99, 3, 'WA'),\n",
    "        (5, 22.00, 3, 'WA'),\n",
    "        (6, 25.00, 7, 'WA'),\n",
    "    ],\n",
    "    [\"cust_id\", 'monthly_payment', 'tenure_yrs', 'state']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80e8bb62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------------+----------+-----+\n",
      "|cust_id|monthly_payment|tenure_yrs|state|\n",
      "+-------+---------------+----------+-----+\n",
      "|1      |29.99          |5         |MD   |\n",
      "|2      |31.99          |3         |MD   |\n",
      "|3      |24.99          |1         |VA   |\n",
      "|4      |21.99          |3         |WA   |\n",
      "|5      |22.0           |3         |WA   |\n",
      "|6      |25.0           |7         |WA   |\n",
      "+-------+---------------+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customerdata1.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a804db",
   "metadata": {},
   "source": [
    "There is a little bit more work involved when we have categorical variable in the data. First step is to change categories to number. And, this is accomplished by using StringIndexer available in pyspark.ml.feature. The StringIndexer allocates unique values to each of the categories of the variable. \n",
    "\n",
    "When StringIndexer is applied, the most frequent category gets the index 0, followed by next most frequent and so on. Below, state : WA gets 0 index as it is the most frequent category in state, followed by MD :1 and VA :2. \n",
    "\n",
    "Let's import required functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4310b2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import (StringIndexer, OneHotEncoder, VectorAssembler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f37bfb7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------------+----------+-----+--------+\n",
      "|cust_id|monthly_payment|tenure_yrs|state|stateNum|\n",
      "+-------+---------------+----------+-----+--------+\n",
      "|1      |29.99          |5         |MD   |1.0     |\n",
      "|2      |31.99          |3         |MD   |1.0     |\n",
      "|3      |24.99          |1         |VA   |2.0     |\n",
      "|4      |21.99          |3         |WA   |0.0     |\n",
      "|5      |22.0           |3         |WA   |0.0     |\n",
      "|6      |25.0           |7         |WA   |0.0     |\n",
      "+-------+---------------+----------+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "indexer = StringIndexer(inputCol='state', outputCol='stateNum')\n",
    "indexd_data=indexer.fit(customerdata1).transform(customerdata1)\n",
    "indexd_data.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "50c8d2a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State distribution\n",
      "+-----+-----+\n",
      "|state|count|\n",
      "+-----+-----+\n",
      "|   VA|    1|\n",
      "|   MD|    2|\n",
      "|   WA|    3|\n",
      "+-----+-----+\n",
      "\n",
      "stateNum distribution\n",
      "+--------+-----+\n",
      "|stateNum|count|\n",
      "+--------+-----+\n",
      "|     0.0|    3|\n",
      "|     1.0|    2|\n",
      "|     2.0|    1|\n",
      "+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('State distribution')\n",
    "indexd_data.groupBy('state').count().show()\n",
    "print('stateNum distribution')\n",
    "indexd_data.groupBy('stateNum').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183cbbf3",
   "metadata": {},
   "source": [
    "As shown above, the StateNum variable is created which has numerical representation for each of the states. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f4f5a1",
   "metadata": {},
   "source": [
    "Once StringIndexer is performed, we can use OneHotEncoder to encode the indexed variable and finally use VectorAssembler to assemble all numerical and one hot encoded vectors together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b7d3dbd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = OneHotEncoder(inputCol='stateNum', outputCol = 'stateVec')\n",
    "onehotdata = encoder.fit(indexd_data).transform(indexd_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8f663236",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------------+----------+-----+--------+-------------+\n",
      "|cust_id|monthly_payment|tenure_yrs|state|stateNum|stateVec     |\n",
      "+-------+---------------+----------+-----+--------+-------------+\n",
      "|1      |29.99          |5         |MD   |1.0     |(2,[1],[1.0])|\n",
      "|2      |31.99          |3         |MD   |1.0     |(2,[1],[1.0])|\n",
      "|3      |24.99          |1         |VA   |2.0     |(2,[],[])    |\n",
      "|4      |21.99          |3         |WA   |0.0     |(2,[0],[1.0])|\n",
      "|5      |22.0           |3         |WA   |0.0     |(2,[0],[1.0])|\n",
      "|6      |25.0           |7         |WA   |0.0     |(2,[0],[1.0])|\n",
      "+-------+---------------+----------+-----+--------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "onehotdata.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6b8a69",
   "metadata": {},
   "source": [
    "The OneHotEcnoder creates dummy variables for each category. In our case state has 3 categories, MD, VA and WA. When used OneHotEncoder, these 3 categories are represented by two columns (vectors). The one hot encoder outputs n-1 vectors where n is number of categoires in a feature (it's 3, in our case for state). One hot encoded vector looks little bit different as shown above in stateVec. It captures number of vectors, value contained in the vector and position of the value. Let's take a closer look at stateVec for MD i.e. (2,[1],[1.0]). It represents, there are 2 vectors (3 categories -1), with value 1 (1.0) at position 1. \n",
    "\n",
    "Furthermore, when we use StringIndexer, the state gets the stateNum 0 for WA (most frequent), 1 for MD (second most) and 2 for VA (least frequent) . Subsequently, when the one hot encoding vector is created, there will be first vector for WA (call WA_vector) where WA_vector=1 for the rows containing WA and 0 otherwise. Similarly, the other vector would be for MD (call MD_vector) where MD_Vector= 1 for the rows containing MD otherwise 0. Now, going back to example (2,[1],[1.0]). We have 2 vectors and for MD, the vector value is 1 at 1st position (0th position for WA, 1st for MD). Similarly for WA the representation is (2,[0],[1.0]). It indicates, 2 vectors, and value of 1 (1.0) is available at 0th place (0th position for WA).  And, for VA it is represented by (2,[],[]).\n",
    "\n",
    "\n",
    "Note: The way spark handles the One hot encoder is that it does not include the last category. For example with 3 categories with input value of 1 would map to an output vector of [0.0,1.0] The last category is not included. Another example of feature with 3 categories with input value of 2 would map to [0.0,0.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138a77ef",
   "metadata": {},
   "source": [
    "#### Vector Assmebler\n",
    "Now that we have converted the categeorical feature to numerical form, we need to assemble all the input columns including this converted one into a single vector called features. As shown below, the output data has all the inputs that we provided in inputCols of VectorAssembler. Next for further downstream uses such as train, test split and modeling, we only use features as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bbe85d63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------------+----------+-----+--------+-------------+-----------------------+\n",
      "|cust_id|monthly_payment|tenure_yrs|state|stateNum|stateVec     |features               |\n",
      "+-------+---------------+----------+-----+--------+-------------+-----------------------+\n",
      "|1      |29.99          |5         |MD   |1.0     |(2,[1],[1.0])|[1.0,29.99,5.0,0.0,1.0]|\n",
      "|2      |31.99          |3         |MD   |1.0     |(2,[1],[1.0])|[2.0,31.99,3.0,0.0,1.0]|\n",
      "|3      |24.99          |1         |VA   |2.0     |(2,[],[])    |[3.0,24.99,1.0,0.0,0.0]|\n",
      "|4      |21.99          |3         |WA   |0.0     |(2,[0],[1.0])|[4.0,21.99,3.0,1.0,0.0]|\n",
      "|5      |22.0           |3         |WA   |0.0     |(2,[0],[1.0])|[5.0,22.0,3.0,1.0,0.0] |\n",
      "|6      |25.0           |7         |WA   |0.0     |(2,[0],[1.0])|[6.0,25.0,7.0,1.0,0.0] |\n",
      "+-------+---------------+----------+-----+--------+-------------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "assembler1 = VectorAssembler(\n",
    "    inputCols=[\"cust_id\", \"monthly_payment\", \"tenure_yrs\", \"stateVec\"],\n",
    "    outputCol=\"features\")\n",
    "outdata1 = assembler1.transform(onehotdata)\n",
    "outdata1.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a5b605",
   "metadata": {},
   "source": [
    "### Summary\n",
    "PySpark modeling requires to prepare data using VectorAssembler which contains all the numerical features and vector converted categorical features. StringIndexer and OneHotEncoder avaialble in pyspark.ml.feature are important steps for converting categorical variable into a vectorized form which then can be used for downstream modeling work. \n",
    "\n",
    "Look out for my next article on ML models using PySpark."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5700a6da",
   "metadata": {},
   "source": [
    "### END END END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
